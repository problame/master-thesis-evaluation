{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import dotted # https://pypi.org/project/dotted-notation/\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import lib.datasciencetoolbelt as dstools\n",
    "from lib.resultstorage import ResultStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstools.setup({\n",
    "    \"seaborn_context\": \"talk\",\n",
    "    \"savefig\": {\n",
    "        \"enable\": True,\n",
    "        \"dir\": Path(\"./postprocess_results\"),\n",
    "    }\n",
    "})\n",
    "result_storage = ResultStorage(Path(\"./results\"))\n",
    "\n",
    "#%matplotlib qt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_storage_prefix = \"itxg_bypass_v5\"\n",
    "\n",
    "id_vars__dottedpath_and_shortname_and_type = [\n",
    "    (\"zfs_setup.module_args.zfs.zfs_zil_itxg_bypass\", \"itxg_bypass\", str),\n",
    "    (\"zfs_setup.module_args.zfs.zvol_request_sync\", \"zvol_request_sync\", str),\n",
    "    (\"zfs_setup.module_args.zfs.zfs_zil_pmem_prb_ncommitters\", \"ncommitters\", int),\n",
    "    (\"fio_config.fsync_every\", \"fsync_every\", int),\n",
    "    (\"fio_config.numjobs\", \"numjobs\", int)\n",
    "]\n",
    "id_vars = [p[1] for p in id_vars__dottedpath_and_shortname_and_type]\n",
    "\n",
    "def extract_id_var_values(output_json):\n",
    "    d = output_json\n",
    "    id_var_values = {}\n",
    "    for dp, sn, ty in id_vars__dottedpath_and_shortname_and_type: \n",
    "        v = dotted.get(d, dp)\n",
    "        if not v:\n",
    "            raise Exception(f\"{d['file']}: dotted path {dp} not found\")\n",
    "        if sn in id_var_values:\n",
    "            raise Exception(f\"duplicate shortname {sn}\")\n",
    "        try:\n",
    "            id_var_values[sn] = ty(v)\n",
    "        except ValueError as e:\n",
    "            raise Exception(f\"cannot parse v={v!r}\") from e\n",
    "    return id_var_values\n",
    "\n",
    "def get_fio_write_metrics(output_json):\n",
    "    d = output_json\n",
    "    jobs = dotted.get(d, \"fio_jsonplus.jobs\")\n",
    "    assert len(jobs) == 1\n",
    "    j0 = jobs[0]\n",
    "    jw = jobs[0][\"write\"]\n",
    "    return jw\n",
    "\n",
    "def to_fio_results_dict(output_json):\n",
    "    jw = get_fio_write_metrics(output_json)\n",
    "    return {\n",
    "        **extract_id_var_values(output_json),\n",
    "        \"w_iops_mean\": jw[\"iops_mean\"],\n",
    "        \"w_iops_stddev\": jw[\"iops_stddev\"],\n",
    "        \"w_lat_mean\": dotted.get(jw, \"lat_ns.mean\"),\n",
    "        \"w_lat_stddev\": dotted.get(jw, \"lat_ns.stddev\"),\n",
    "    }\n",
    "\n",
    "def to_kstat_results_dict(output_json):\n",
    "    d = output_json\n",
    "    return {\n",
    "        **extract_id_var_values(output_json),\n",
    "        **d[\"zvol_stats\"],\n",
    "        **d[\"itxg_bypass_stats\"],\n",
    "        **d[\"zil_pmem_stats\"],\n",
    "        **d[\"zil_pmem_ringbuf_stats\"],\n",
    "        \"bio_total\": d[\"zvol_stats\"][\"submit_bio__zvol_write(with_taskq_if_enabled)\"],\n",
    "        \"taskq_delay\": dotted.get(d, 'zvol_stats.zvol_write__taskq_qdelay'),\n",
    "        \"assign_aquire\": dotted.get(d, 'itxg_bypass_stats.assign__aquisition_total'),\n",
    "        \"assign_vtable\": dotted.get(d, 'itxg_bypass_stats.assign__vtable'),\n",
    "        \"assign_total\": dotted.get(d, 'itxg_bypass_stats.assign__total'),\n",
    "        \"commit_total\": dotted.get(d, 'itxg_bypass_stats.commit__total'),\n",
    "        \"commit_aquire\": dotted.get(d, 'itxg_bypass_stats.commit__aquire'),\n",
    "        \n",
    "    }\n",
    "\n",
    "def to_cpu_dict(output_json):\n",
    "    d = output_json\n",
    "    return {\n",
    "        **extract_id_var_values(output_json),\n",
    "        **{f\"cpu_{comp}\": val for comp, val in dotted.get(d, \"cpu_time.allcpu\").items()},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute `df_kstats`\n",
    "rows = [to_kstat_results_dict(j) for j in result_storage.iter_results(result_storage_prefix)]\n",
    "df_kstats = pd.DataFrame.from_dict(rows).set_index(id_vars).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute `df_cpu`\n",
    "rows = [to_cpu_dict(j) for j in result_storage.iter_results(result_storage_prefix)]\n",
    "df = pd.DataFrame.from_dict(rows)\n",
    "df = df.set_index(id_vars).sort_index()\n",
    "df = df.rename_axis(\"metric\", axis=1)\n",
    "df = df.stack()\n",
    "df_cpu = df\n",
    "del df\n",
    "df_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## derive `df_cpu.notidle`\n",
    "tmp = df_cpu.unstack(\"metric\")\n",
    "tmp[\"cpu_not_idle\"] = tmp.sum(axis=1) - tmp.cpu_idle\n",
    "df_cpu = tmp.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute `df`\n",
    "rows = [to_fio_results_dict(j) for j in result_storage.iter_results(result_storage_prefix)]\n",
    "df = pd.DataFrame.from_dict(rows)\n",
    "df = df.set_index(id_vars).sort_index()\n",
    "df = df.rename_axis(\"metric\", axis=1)\n",
    "df = df.stack()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick peek on the actual data in `df`\n",
    "df.unstack(\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define df_zfssetup\n",
    "data = df.unstack([\"itxg_bypass\", \"zvol_request_sync\", \"ncommitters\"])\n",
    "data.columns = data.columns.map(lambda x: f\"zil-pmem bypass={ {'1':'yes', '0': 'no'}[x[0]]} zvol_taskq={ {'1':'no', '0':'yes'}[x[1]] } ncommitters={x[2]}\")\n",
    "data = data.rename_axis(\"zfs_setup\", axis=1)\n",
    "data = data.stack()\n",
    "data\n",
    "df_zfssetup = data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def filter_by_index_value(df, level, value):\n",
    "    \"\"\"Reduce dimensionality of a dataframe by filtering by and subsequently dropping one of its index levels.\n",
    "    \n",
    "    df is assumed to be a multi-indexed pd.DataFrame.\n",
    "    First, filter the data frame so that we only keep rows whose index tuple has value `value` at level `level`.\n",
    "    Now the resulting data frame only has a single value at the level.\n",
    "    Thus remove that level from the index.\n",
    "    Voila: dimensionality reduced.\n",
    "    \"\"\"\n",
    "    df = df[df.index.get_level_values(level) == value]\n",
    "    assert set(df.index.get_level_values(level)) == {value}\n",
    "    df.index = df.index.droplevel(level)\n",
    "    return df\n",
    "\n",
    "def _test_filter_by_index_value():\n",
    "    data = [{\"favnum\": n, \"favletter\": l, \"id\": id} for id, (n, l) in enumerate(itertools.product([23,42],[\"a\", \"b\"]))]\n",
    "    d = pd.DataFrame(data).set_index([\"favnum\", \"favletter\"])\n",
    "    display(d)\n",
    "    display(filter_by_index_value(d, \"favnum\", 23))\n",
    "    display(filter_by_index_value(d, \"favletter\", \"b\"))\n",
    "    \n",
    "_test_filter_by_index_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_values_sorted_unique(df, level):\n",
    "    \"\"\"Returns the sorted unique values of a DataFrame's multi-index at level `level`\"\"\"\n",
    "    return sorted(list(set(df.index.get_level_values(level))))\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "class FactorizedDataFrameItem(AttrDict):\n",
    "    @property\n",
    "    def title(self):\n",
    "        return f\"{self.fdf.row}={self.rv}|{self.fdf.col}={self.cv}\"\n",
    "        \n",
    "        \n",
    "class FactorizedDataFrame:\n",
    "    def __init__(self, data, row, col):\n",
    "        self.data = data\n",
    "        self.col = col\n",
    "        self.row = row\n",
    "\n",
    "        self.col_values = level_values_sorted_unique(self.data, self.col)\n",
    "        self.row_values = level_values_sorted_unique(self.data, self.row)\n",
    "        \n",
    "    def iter_factorized(self):\n",
    "        for ci, c in enumerate(self.col_values):\n",
    "            for ri, r in enumerate(self.row_values):\n",
    "                d = self.data.copy()\n",
    "                d = filter_by_index_value(d, self.col, c)\n",
    "                d = filter_by_index_value(d, self.row, r)\n",
    "                # display(d)\n",
    "            \n",
    "                context = FactorizedDataFrameItem({\n",
    "                    \"fdf\": self,\n",
    "                    \"d\": d,\n",
    "                    \"ri\": ri,\n",
    "                    \"rv\": r,\n",
    "                    \"ci\": ci,\n",
    "                    \"cv\": c,\n",
    "                })\n",
    "                yield context\n",
    "                \n",
    "\n",
    "def factorplot(data=None, row=None, col=None, plot=None, subplots_kw={}):\n",
    "    \"\"\"Factorizez MultiIndex'ed DataFrame `data`, then invokes `plot` for each FactorizedDataFrameItem\"\"\"\n",
    "    \n",
    "    fdf = FactorizedDataFrame(data, row, col)\n",
    "    \n",
    "    subplots_kw = {\n",
    "        \"gridspec_kw\": {'hspace': 1},\n",
    "        **subplots_kw,\n",
    "        \"squeeze\": False, # axes should always be two-dimensional\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(len(fdf.row_values), len(fdf.col_values), **subplots_kw)\n",
    "\n",
    "    for f in fdf.iter_factorized():\n",
    "        ax = axes[f.ri, f.ci]\n",
    "        ax.set_title(f.title)\n",
    "        legend = f.ri == len(fdf.row_values)-1 and f.ci == len(fdf.col_values)-1\n",
    "        plot(f.d, ax, legend)\n",
    "        if legend:\n",
    "            plt.legend(loc='lower left', bbox_to_anchor=(1,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_idvars(data):\n",
    "    data = filter_by_index_value(data, 'zvol_request_sync', \"1\")\n",
    "    data = filter_by_index_value(data, 'itxg_bypass', \"1\")\n",
    "    data = data.query('ncommitters in [1,2,4,6,8,12,16,24] and fsync_every in [1,16] and numjobs <= 16')\n",
    "    return data\n",
    "    \n",
    "def make_data(normalize=None):\n",
    "\n",
    "    # compute latency breakdown variables\n",
    "    data = df_kstats.copy()\n",
    "    data['t_taskq'] = data.zvol_write__taskq_qdelay\n",
    "    data['t_bypass_commit'] = data.commit_total\n",
    "    data['t_bypass_assign_aquisition'] = data.assign__aquisition_total\n",
    "    data['t_bypass_assign_exit'] = data.assign__exit\n",
    "    data['t_prb'] = data.write_entry_time - data.prb_write__pmem\n",
    "    data['t_pmem'] = data.prb_write__pmem\n",
    "    data['t_get_data'] = data.get_data_time\n",
    "    data['t_zil'] = data.assign__vtable - data.write_entry_time\n",
    "    data['t_zvol_write'] = data.bio_total - (data.zvol_write__taskq_qdelay + data.zvol_write__1zil_commit + data.zvol_write__zvol_log_write_finish + data.zvol_write__2zil_commit)\n",
    "    \n",
    "    components = ['t_taskq', 't_bypass_commit', 't_bypass_assign_aquisition',\n",
    "                  't_bypass_assign_exit', 't_prb', 't_pmem', 't_get_data', 't_zil', 't_zvol_write']\n",
    "    \n",
    "    # t_other is what we didn't account for yet\n",
    "    data['t_other'] = data.bio_total - data[components].sum(axis=1)\n",
    "    components += ['t_other']\n",
    "\n",
    "    data = normalize(data)\n",
    "\n",
    "    # project to components\n",
    "    data = data[components]\n",
    "\n",
    "\n",
    "    data = data.rename_axis('component', axis=1)\n",
    "    data = data.stack()\n",
    "    data = data.rename(\"rel_time\")\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    data = filter_idvars(data)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "xlim=(0,17)\n",
    "xticks=range(0,17, 2)\n",
    "\n",
    "####\n",
    "print(\"relative latency breakdown\")\n",
    "\n",
    "def plot_latency_breakdown(data, ax, legend):\n",
    "#     display(data)\n",
    "    data = data.unstack(\"component\")\n",
    "    data.plot.area(ax=ax, legend=legend, xlim=xlim, xticks=xticks, ylim=(-0.1, 1.1))\n",
    "\n",
    "data_lb = make_data(normalize=lambda data: data.div(data.bio_total, axis=0))\n",
    "factorplot(data=data_lb, col='ncommitters', row='fsync_every', plot=plot_latency_breakdown,\n",
    "          subplots_kw={\n",
    "            \"figsize\": (30, 10),\n",
    "            \"gridspec_kw\": {\n",
    "                \"hspace\": 0.2,\n",
    "            },\n",
    "        })\n",
    "\n",
    "plt.show()\n",
    "\n",
    "####\n",
    "print(\"\"\"\n",
    "Absolute latencies divided by numjobs, EXCEPT t_pmem.\n",
    "\n",
    "Rationale: This section of the evaluation is about evaluating the PMEM write overhead limiter.\n",
    "Reminder: Writing to PMEM above its write bandwidth limit wastes CPU time because the write\n",
    "          instructions stall waiting for PMEM.\n",
    "This means that an ideal implementation would allow t_pmem to reach the value where we achieve\n",
    "peak performance (IOPS in our case), then keep the value at that level by queuing additional\n",
    "writers on the CPU (we do it using a semaphore in t_prb).\n",
    "Thus, an ideal t_pmem should look like so:\n",
    "\n",
    "   _____________\n",
    "  /\n",
    " /\n",
    "/\n",
    "\n",
    "If we divided t_pmem by `numjobs` like all the other `t_` metrics, that straight line would\n",
    "need to turn into a declining line, proportional to `numjobs`.\n",
    "That's quite hard to spot, so we felt it's better to not scale t_pmem at all.\n",
    "\"\"\")\n",
    "\n",
    "def plot_latency_curves(data, ax, legend):\n",
    "#     display(data)\n",
    "    data = data.unstack(\"component\")\n",
    "#     display(data)\n",
    "    #data.plot.area(ax=ax, legend=legend, xlim=xlim, xticks=xticks)\n",
    "    data.plot.line(ax=ax, legend=legend, xlim=xlim, xticks=xticks, ylim=(0, 3e10))\n",
    "    \n",
    "def normalize_by_numjobs_except_t_pmem(data):\n",
    "    tmp = data.reset_index(level='numjobs')\n",
    "    numjobs_orig = tmp.numjobs\n",
    "    tmp = tmp.div(tmp.numjobs, axis=0)\n",
    "    tmp['t_pmem'] = tmp['t_pmem'].mul(numjobs_orig) # !!!!!!!!!!!! don't normalize t_pmem\n",
    "    tmp['numjobs'] = numjobs_orig\n",
    "    tmp = tmp.set_index('numjobs', append=True)\n",
    "    return tmp\n",
    "\n",
    "data_abs = make_data(normalize=normalize_by_numjobs_except_t_pmem)\n",
    "factorplot(data=data_abs, col='ncommitters', row='fsync_every', plot=plot_latency_curves,\n",
    "          subplots_kw={\n",
    "            \"figsize\": (30, 10),\n",
    "            \"gridspec_kw\": {\n",
    "                \"hspace\": 0.2,\n",
    "            },\n",
    "        })\n",
    "\n",
    "plt.show()\n",
    "\n",
    "####\n",
    "\n",
    "def plot_perf_metric(metric):\n",
    "\n",
    "    full_data = df.unstack(\"metric\")\n",
    "    full_data = filter_idvars(full_data)\n",
    "    full_data = full_data[[metric]]\n",
    "    ymax = full_data[metric].max() * 1.1\n",
    "\n",
    "    def plot(data, ax, legend):\n",
    "    #     display(data)\n",
    "        #     data = data.pivot(\"numjobs\", \"metric\", \"value\")\n",
    "        #data.plot.line(ax=ax, legend=legend, xlim=(0, None), ylim=(0, context[\"full_data\"][metric].max()))\n",
    "        data.plot.line(ax=ax, legend=legend, xlim=xlim, xticks=xticks, ylim=(0, ymax))\n",
    "\n",
    "    factorplot(data=full_data, col='ncommitters', row='fsync_every',\n",
    "              plot=plot,\n",
    "              subplots_kw={\n",
    "                \"figsize\": (30, 5),\n",
    "                \"gridspec_kw\": {\n",
    "                    \"hspace\": 1,\n",
    "                }\n",
    "              })\n",
    "    plt.show()\n",
    "\n",
    "print(\"IOPS mean\")\n",
    "plot_perf_metric(\"w_iops_mean\")\n",
    "print(\"IOPS std\")\n",
    "plot_perf_metric(\"w_iops_stddev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- We only look at the lower row (`fsync_every = 16`) because that's where actual parallelism is available.\n",
    "- We also only look at `numjobs \\in [0, ..., 8]` as we only have 8 cores (16 SMTs).\n",
    "- The PMEM overload problem is clearly visible:\n",
    "  - We already achieve close-to-peak IOP performance at `ncommitters=2 and numjobs=6`.\n",
    "  - And if we allow `ncommiters > 2` we can sustain those IOPS but at the cost of higher `t_pmem`.\n",
    "  - That `t_pmem` time is time spent on-CPU, i.e., wasting CPU time.\n",
    "    In comparison, for `ncommitters == 2` and rising `numjobs`, `t_prb` becomes larger, which is where we queue the threads waiting for a committer slot (i.e. the PMEM overload protection feature). (Look at the relative distribution and see how t_pmem shifts to t_prb as `numjobs` grows for `ncommitters == 2`.\n",
    "- => the PMEM overload protection feature works.\n",
    "- But the `itxg_bypass` implementation has poor multicore scalability.\n",
    "  - `t_itxg_bypass*` are all divided by `numjobs`.\n",
    "  - `t_itxg_bypass_assign_aquisition` rises as `numjobs` increase.\n",
    "  - => `t_itxg_bypass_assign_aquisition` does not scale linearly with number of cores.\n",
    "- The rising per-thread cost of the `itxg_bypass` slows `t_pmem` down at high `numjobs`.\n",
    "  - Already visible for `numjobs \\in [0, ..., 8]`\n",
    "  - The eventual downturn of `t_pmem` at `numjobs > 8` might be caused by this as well (time-expenditure shifts to worse-and-worse scaling `itxg_bypass`, relieving pressure from PMEM)\n",
    "  \n",
    "=> conclusion for the next experiment:\n",
    "- add a mode where the bypass's scaling behavior is avoided so that we can isolate the scalability of the PMEM overload protection feature\n",
    "  - => add `itxg_bypass=2` where we do not use the rwlock at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What overhead does the zvol taskq have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_kstats.query('fsync_every in [1,4,16]')\n",
    "data = data.query('zvol_request_sync == \"0\"')\n",
    "data = data.reset_index()\n",
    "# commit__total because it's essentially a single `mov` between aquisition and exit\n",
    "data['rel_taskq_delay'] = data.zvol_write__taskq_qdelay / data.bio_total\n",
    "g = sns.relplot(data=data,\n",
    "            kind='line',\n",
    "            height=5,\n",
    "            ci='std',\n",
    "            col='fsync_every',\n",
    "            row='itxg_bypass',\n",
    "#             hue='itxg_bypass',\n",
    "            hue='ncommitters',\n",
    "            markers=True,\n",
    "            x ='numjobs', y='rel_taskq_delay')\n",
    "g.set(ylim=(-0.1,1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is relative overhead!\n",
    "- In general: for both itxg_bypass values it's an unacceptably high overhead for something so simple\n",
    "  => suggests that there is some scalability bottleneck\n",
    "- The breakdowns for small ncommitters are due to the fact that waiting for a committer slot becomes the bottleneck in the whole equation.\n",
    "- **A better viz would be repeating the latency breakdown graphs from the previous section but for the latencies involved with zvol_request_sync == 0** => needs different latency computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some metrics in a different form (did'n't lead anywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " g = sns.relplot(\n",
    "        data=filter_idvars(df.unstack('metric').reset_index()),\n",
    "        kind='line',\n",
    "        row='fsync_every',\n",
    "        col='ncommitters',\n",
    "        x='numjobs',\n",
    "        y='w_iops_mean',\n",
    "    )\n",
    "g = sns.relplot(\n",
    "        data=filter_idvars(df.unstack('metric').reset_index()),\n",
    "        kind='line',\n",
    "        row='fsync_every',\n",
    "        col='ncommitters',\n",
    "        x='numjobs',\n",
    "        y='w_lat_mean',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " g = sns.relplot(\n",
    "        data=filter_idvars(df.unstack('metric').reset_index()),\n",
    "        kind='line',\n",
    "        row='fsync_every',\n",
    "        col='ncommitters',\n",
    "        x='numjobs',\n",
    "        y='w_iops_mean',\n",
    "    )\n",
    "g = sns.relplot(\n",
    "        data=filter_idvars(df.unstack('metric').reset_index()),\n",
    "        kind='line',\n",
    "        row='fsync_every',\n",
    "        col='ncommitters',\n",
    "        x='numjobs',\n",
    "        y='w_lat_mean',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kstats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filter_data(data=None):\n",
    "    data = data.copy()\n",
    "    data = data.query(\"itxg_bypass == '1' and zvol_request_sync == '1' and fsync_every in [16]\")\n",
    "    data = data.query('ncommitters in [1,2,3,4,6,8,16] and numjobs <= 16')\n",
    "    return data\n",
    "\n",
    "def filter_and_plot(data=None, y=None):\n",
    "    data = filter_data(data)\n",
    "    data = data.reset_index()\n",
    "    data = data.copy()\n",
    "    data['fsync_every'] = data.fsync_every.astype('category')\n",
    "    g = sns.relplot(\n",
    "        data=data,\n",
    "        kind='line',\n",
    "#         col='fsync_every',\n",
    "        col='ncommitters',\n",
    "        x='numjobs',\n",
    "        y=y,\n",
    "        style='fsync_every', hue='fsync_every'\n",
    "    )\n",
    "    return g\n",
    "\n",
    "sdf = df_kstats.copy()\n",
    "sdf['get_committer_slot_lat'] =  sdf.prb_write__get_committer_slot / sdf.write_entry_count # FIXME: separate count metric\n",
    "sdf['avg_pmem_write_lat'] = sdf.prb_write__pmem / sdf.write_entry_count # FIXME: separate count metric\n",
    "\n",
    "pdf = df.copy()\n",
    "pdf = pdf.unstack(\"metric\")\n",
    "pdf['rel_lat_pmem_vs_fio'] = sdf['avg_pmem_write_lat'] / pdf['w_lat_mean']\n",
    "pdf['rel_lat_committerslot_vs_fio'] = sdf['get_committer_slot_lat'] / pdf['w_lat_mean']\n",
    "\n",
    "tmp = pdf.reset_index(level='numjobs')\n",
    "tmp['w_iops_mean_per_thread'] = tmp.w_iops_mean / tmp.numjobs\n",
    "tmp = tmp.set_index('numjobs', append=True)\n",
    "pdf['w_iops_mean_per_thread'] = tmp['w_iops_mean_per_thread']\n",
    "\n",
    "pdf['iops_for_latency'] = pdf['w_iops_mean'] / pdf.w_lat_mean\n",
    "\n",
    "cdf = df_cpu.copy().unstack('metric')\n",
    "cdf['cpu_total'] = cdf.cpu_not_idle + cdf.cpu_idle\n",
    "cdf['cpu_isolcpus_idle'] = cdf.cpu_total / 2 # 50% of cores were disabled using isolcpus= => their time doesn't count\n",
    "cdf['cpu_nonisol_total'] = cdf.cpu_total - cdf.cpu_isolcpus_idle\n",
    "cdf['cpu_nonisol_utilization'] = cdf.cpu_not_idle / cdf.cpu_nonisol_total\n",
    "\n",
    "display(cdf.cpu_nonisol_utilization.describe())\n",
    "print(\"note: SMT is still active, thus 50% or more _could_ mean 100% busy bars for the SMT thread of one core in htop. In practice this _is_ always the case.\")\n",
    "\n",
    "pdf['cpu_time_per_iops'] = 1e9 * ((cdf.cpu_not_idle) / pdf.w_iops_mean)\n",
    "\n",
    "tmp = pdf.reset_index(level='numjobs')\n",
    "tmp['cpu_time_per_iops_by_thread'] = tmp.cpu_time_per_iops / tmp.numjobs\n",
    "tmp = tmp.set_index('numjobs', append=True)\n",
    "pdf['cpu_time_per_iops_by_thread'] = tmp.cpu_time_per_iops_by_thread\n",
    "\n",
    "tmp = cdf.reset_index(level='numjobs')\n",
    "tmp['cpu_time_per_thread'] = tmp.cpu_not_idle / tmp.numjobs\n",
    "tmp = tmp.set_index('numjobs', append=True)\n",
    "cdf['cpu_time_per_thread'] = tmp.cpu_time_per_thread\n",
    "\n",
    "# sdf['pmem_time_per_iops'] = ((sdf.prb_write__pmem) / (pdf.w_iops_mean))\n",
    "sdf['pmem_time_per_cpu_nonisol_total'] = (sdf.prb_write__pmem / 1e9) / (cdf.cpu_nonisol_total)\n",
    "# sdf['bypass_time_per_cpu_nonisol_total'] = ((df_latencybreakdown.t_bypass_assign_aquisition + df_latencybreakdown.t_bypass_assign_exit + df_latencybreakdown.t_bypass_commit)/1e9) / cdf.cpu_nonisol_total\n",
    "\n",
    "g = filter_and_plot(data=cdf, y='cpu_nonisol_utilization')\n",
    "g.set(ylim=(-0.1, 1.1))\n",
    "g = filter_and_plot(data=sdf, y='pmem_time_per_cpu_nonisol_total')\n",
    "g.set(ylim=(-0.1, 1.1))\n",
    "\n",
    "\n",
    "filter_and_plot(data=pdf, y='w_iops_mean')\n",
    "#filter_and_plot(data=pdf, y='w_iops_mean_per_thread')\n",
    "\n",
    "g = filter_and_plot(data=pdf, y='cpu_time_per_iops_by_thread')\n",
    "g.set(ylim=(0, None))\n",
    "g = filter_and_plot(data=cdf, y='cpu_time_per_thread')\n",
    "g.set(ylim=(0, None))\n",
    "\n",
    "# filter_and_plot(data=sdf, y='pmem_time_per_iops')\n",
    "# g = filter_and_plot(data=sdf, y='bypass_time_per_cpu_nonisol_total')\n",
    "# g.set(ylim=(-0.1, 1.1))\n",
    "\n",
    "\n",
    "filter_and_plot(data=sdf, y='avg_pmem_write_lat')\n",
    "\n",
    "\n",
    "# filter_and_plot(data=pdf, y='iops_for_latency')\n",
    "\n",
    "# g = filter_and_plot(data=pdf, y='rel_lat_pmem_vs_fio')\n",
    "# g.set(ylim=(-0.1,0.5))\n",
    "\n",
    "# g = filter_and_plot(data=pdf, y='rel_lat_committerslot_vs_fio')\n",
    "# g.set(ylim=(-0.1,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `cpu_time_per_iops` and `avg_pmem_write_lat` have correlating edges. These edges move toward the right as `ncommitters` increases.\n",
    "  - Both curves smoothen as `ncommitters` grows between the lower left and the edge point.\n",
    "  - With this hardware configuration (single DIMM) A value of `ncommitters` between 1 and 2 yields near optimal performance (>300k IOPS) while not wasting CPU time on PMEM stall\n",
    "  - For higher `ncommitters` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
