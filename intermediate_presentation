#!/usr/bin/env python3

import lib.cpus
import lib.pmem
import lib.partitioning
import lib.zfssetup
import lib.fio
import lib.zfs_write_latencyanalysis
from lib.resultstorage import ResultStorage
from pathlib import Path
import hashlib
import tempfile
import json
import shutil
import sys
from lib.helpers import merge_dicts, string_with_one_format_placeholder
import copy
import functools
import threading
import time
import queue

class Store:
	d = {}
	def add(self, configlabel, value):
		assert isinstance(configlabel, str)
		assert isinstance(value, str)
		l = self.d.get(configlabel, [])
		l.append(value)
		self.d[configlabel] = l

	def get_one(self, configlabel):
		assert isinstance(configlabel, str)
		l = self.d.get(configlabel, [])
		assert len(l) == 1
		return l[0]

	def get_all(self, configlabel):
		assert isinstance(configlabel, str)
		return self.d.get(configlabel, [])

	def to_dict(self):
		return copy.deepcopy(self.d)

	def from_dict(d):
		for k, v in d.items():
			assert isinstance(k, str)
			assert isinstance(v, list)
			for i in v:
				assert isinstance(i, str)
		s = Store()
		s.d = d
		return s

resultdir = Path("./results")
store_savepoint_filepath = resultdir / "store-savepoint.json"
if store_savepoint_filepath.exists():
    with open(store_savepoint_filepath, "r") as f:
        d = json.load(f)
        store = Store.from_dict(d)
        store_loaded_from_savepoint = True
else:
    store = Store()
    store_loaded_from_savepoint = False

def system_setup():
    lib.pmem.setup_pmem({
    		"regions": [
    			{
    				"PersistentMemoryType": "AppDirectNotInterleaved",
    				"SocketID": "0x0000",
    				"DimmID": "0x0020",
    				"namespaces": [
    					{
    						"mode": "devdax",
    						"size": 10 * (1<<30),
    						"configlabel": "devdax",
    					},
    					{
    						"mode": "fsdax",
    						"size": 10 * (1<<30),
    						"configlabel": "fsdax",
    					},
    				]	
    			}
    		]
    	}, store)

    # FIXME: we can't use dynamic offline/online because fio throws errors: (clock: sched_setaffinity error)
    # => use isolcpus kernel command line parameter for now FIXME check that it's set?
    #cpus = lib.cpus.sysfs_cpus_by_number()
    #lib.cpus.online_offline_cpus(cpus, {
    #            **{i: { "online": True }   for i in range(0, 8)},
    #            **{i: { "online": False }   for i in range(8,16)},
    #            **{i: { "online": True }   for i in range(16,24)},
    #            **{i: { "online": False }   for i in range(24,32)},
    #    })

    PARTITIONINGS = {
        "default": {
            store.get_one("fsdax"): { "noparts": True, "configlabel": "pmemdevice" },
            "/dev/nvme1n1": { "nparts": 10, "configlabel": "nvmepart" },
            "/dev/nvme2n1": { "nparts": 10, "configlabel": "nvmepart" },
            "/dev/nvme3n1": { "nparts": 10, "configlabel": "nvmepart" },
        }
    }

    for dev, config in PARTITIONINGS["default"].items():
        lib.partitioning.partition_disk({"devfspath": dev, **config}, store)


def make_zfs_setup_config(customizations):
    common = {
       "builddir": Path("/root/zil-pmem/zil-pmem"),
       "module_args": {
                "zfs": {
                    #"zfs_nocacheflush": "0",
                    #"zfs_zio_taskq_batch_cpu_pct": "1",
                    "zvol_request_sync": "1",
                },
            },
       "pool_properties": {},
       "filesystem_properties": {
           "recordsize": "4k",
           "compression": "off",
       },
       "poolname":"dut",
       "mountpoint": Path("/dut"),
       "vdevs": [
           *store.get_all("nvmepart"),
            "log",
       ],
       "create_child_datasets": {
           "dirname_format_str": "ds{}",
           "name_format_str": "ds{}",
           "count": 32,
       },
       "create_child_zvols": {
           "name_format_str": "zv{}",
           "count": 4,
           "size": "4G",
           "volblocksize": 4096,
       },
    }
    return merge_dicts(common, customizations)

def make_fio_config(fio_config_constomizations):
    return merge_dicts({
			"fio_binary": "/usr/local/bin/fio",
            "blocksize": 1<<12, # keep in sync with zfs recordsize prop!
            "size": 200 * (1<<20), # small size so that data fits into the ARC / dbuf caches
            "runtime_seconds": "10",
            "ramp_seconds": 2,
    }, fio_config_constomizations)


def run_fio_series(fio_config, emit_result):
    for numjobs in range(1,16):
        assert "numjobs" not in fio_config
        this_fio_config = merge_dicts(fio_config, { "numjobs": numjobs })
        fiojson = lib.fio.run(this_fio_config)
        result = {
                "fio_config": this_fio_config,
                "fio_jsonplus": fiojson,
        }
        emit_result(result)


def zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result):
	with lib.zfssetup.setup_openzfs(zfs_setup_config) as _:
            run_fio_series(fio_config, lambda r: emit_result(merge_dicts(r, {"zfs_setup": zfs_setup_config})))

def make_fio_zfs_config(fio_customizations):
    return make_fio_config(merge_dicts(
        {
            "target": {
                "type": "fs",
                "filename_format_str": "/dut/ds{}/benchmark",
                "require_filename_format_str_parent_is_mountpoint": True,
                "prewrite_mode": "delete",
            },
        },
        fio_customizations,
    ))

def zilpmem_benchmark(emit_result):
    zfs_setup_config = make_zfs_setup_config({
        "module_args": {
            "zfs": {
                    "zil_default_kind": "2",
                    "zfs_zil_pmem_prb_ncommitters": "8",
            }
        },
        "vdevs": [
            "dax:" + store.get_one('fsdax'),
        ],
    })
    fio_config = make_fio_zfs_config({"sync": 1})
    zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result)

def zillwb_configs(fio_customizations):
    zfs_setup_config = make_zfs_setup_config({
        "module_args": {
            "zfs": {
                    "zil_default_kind": "1",
            }
        },
        "vdevs": [
            "nodax:" + store.get_one('fsdax'),
        ],
    })
    fio_config = make_fio_zfs_config(fio_customizations)
    return (zfs_setup_config, fio_config)


def zillwb_benchmark(emit_result):
    zfs_setup_config, fio_config = zillwb_configs({ "sync": 1 })
    zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result)

def sync0_benchmark(emit_result):
    zfs_setup_config, fio_config = zillwb_configs({ "sync": 0 })
    zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result)

def devdax_benchmark(emit_result):
    run_fio_series(make_fio_config({
        "sync": 1,
        "target": {
            "type": "devdax",
            "devdax_path": store.get_one('devdax'),
        }
    }), emit_result)

def fsdax_benchmark(emit_result):
    run_fio_series(make_fio_config({
        "sync": 1,
        "target": {
            "type": "blockdev",
            "blockdev_path": store.get_one('fsdax'),
        }
    }), emit_result)



def zillwb_latency_analysis(emit_result):

    zfs_setup_config, fio_config_without_numjobs = zillwb_configs({
        "sync": 1,
    })

    with lib.zfssetup.setup_openzfs(zfs_setup_config) as _:

        for numjobs in range(1, 16):
            fio_config = merge_dicts(fio_config_without_numjobs, {"numjobs": numjobs})

            latencyanalysis_config = {
                    "zfs_log_write_kind": "zil-pmem",
            }
            with lib.zfs_write_latencyanalysis.with_bpftrace(latencyanalysis_config) as bpft:
                result_queue = queue.Queue()
                def startstop_measuring():
                    time.sleep(2) # keep in sync with ramp_seconds
                    bpft.start_measuring()
                    time.sleep(8) # keep below runtime_seconds
                    result = bpft.stop_measuring()
                    result_queue.put(result)

                ssmt = threading.Thread(target=startstop_measuring)
                ssmt.start()
                try:
                    lib.fio.run(fio_config)
                except:
                    print("exception running fio")
                    raise
                finally:
                    ssmt.join()
                    result = result_queue.get()
                    assert result != None
                    emit_result(
                        {
                            "zfs_setup": zfs_setup_config,
                            "fio_config": fio_config,
                            "latency_analysis": result,
                        }
                    )

################# MAIN SCRIPT ##################


if not store_loaded_from_savepoint:
    system_setup()
with open(store_savepoint_filepath, "w") as f:
    json.dump(store.to_dict(), f)

result_storage = ResultStorage(resultdir)

def save_json_result(subject, prefix, result_dict):
    result_dict = merge_dicts(result_dict, {"test_subject": subject})
    result_storage.save_json_result(prefix, result_dict)

# perf benchmarks
zilpmem_benchmark(lambda rd: save_json_result("zil-pmem", "perf", rd))
zillwb_benchmark(lambda rd: save_json_result("zil-lwb", "perf", rd))
sync0_benchmark(lambda rd: save_json_result("async", "perf", rd))
devdax_benchmark(lambda rd: save_json_result("devdax", "perf", rd))
fsdax_benchmark(lambda rd: save_json_result("fsdax", "perf", rd))

# latency breakdown
zillwb_latency_analysis(lambda rd: save_json_result(None, "latency_analysis", rd))
