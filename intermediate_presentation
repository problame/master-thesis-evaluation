#!/usr/bin/env python3

import contextlib
import lib.isolcpus
import lib.cpus
import lib.pmem
import lib.partitioning
import lib.zfssetup
import lib.fio
import lib.rocksdb_bench
import lib.filebench
import lib.redis_benchmark
import lib.sqlite_bench
import lib.sysbench_mariadb
import lib.cpu_stats
import lib.zfs_write_latencyanalysis
import lib.zfs_kstats
import lib.devicemapper
import lib.storage_stacks
import lib.benchmarks
from lib.resultstorage import ResultStorage
from pathlib import Path
import hashlib
import tempfile
import json
import shutil
import sys
from lib.helpers import merge_dicts, string_with_one_format_placeholder, json_dump_default_to_str
import copy
import functools
import threading
import time
import queue
import itertools

class Store:
	d = {}
	def add(self, configlabel, value):
		assert isinstance(configlabel, str)
		assert isinstance(value, str)
		l = self.d.get(configlabel, [])
		l.append(value)
		self.d[configlabel] = l

	def get_one(self, configlabel):
		assert isinstance(configlabel, str)
		l = self.d.get(configlabel, [])
		assert len(l) == 1
		return l[0]

	def get_all(self, configlabel):
		assert isinstance(configlabel, str)
		return self.d.get(configlabel, [])

	def get_first(self, configlabel):
		assert isinstance(configlabel, str)
		all = self.get_all(configlabel)
		assert len(all) >= 1
		return all[0]

	def to_dict(self):
		return copy.deepcopy(self.d)

	def from_dict(d):
		for k, v in d.items():
			assert isinstance(k, str)
			assert isinstance(v, list)
			for i in v:
				assert isinstance(i, str)
		s = Store()
		s.d = d
		return s

store = Store()

def system_setup__manual():
    lib.isolcpus.assert_effectively_singlesocket_system(0)

    store.add("fsdax", "/dev/pmem0")
    for i in [1,2,3,4]:
        store.add("blockdevice", f"/dev/pmem{i}")

def system_setup__i30pc61_single_dimm():
    lib.isolcpus.assert_effectively_singlesocket_system(0)

    lib.pmem.setup_pmem({
    	"regions": [
    		{
    			"PersistentMemoryType": "AppDirectNotInterleaved",
    			"SocketID": "0x0000",
    			"DimmID": "0x0020",
    			"namespaces": [
    				{
    					"mode": "devdax",
    					"size": 10 * (1<<30),
    					"configlabel": "devdax",
    				},
    				{
    					"mode": "fsdax",
    					"size": 10 * (1<<30),
    					"configlabel": "fsdax",
    				},
    			]	
    		}
    	]
    }, store)

    partitioning = {
        store.get_one("fsdax"): { "noparts": True, "configlabel": "pmemdevice" }, # why?
        "/dev/nvme1n1": { "nparts": 10, "configlabel": "blockdevice" },
        "/dev/nvme2n1": { "nparts": 10, "configlabel": "blockdevice" },
        "/dev/nvme3n1": { "nparts": 10, "configlabel": "blockdevice" },
    }
    for dev, config in partitioning.items():
        lib.partitioning.partition_disk({"devfspath": dev, **config}, store)

def system_setup__i30pc62():
    lib.isolcpus.assert_effectively_singlesocket_system(0)

    lib.pmem.setup_pmem({
    	"regions": [
            # the dimms on socket 0 are interleaved
    		{
    			"PersistentMemoryType": "AppDirect",
    			"SocketID": "0x0000",
    			"DimmID": "0x0001, 0x0011, 0x0101, 0x0111",
    			"namespaces": [
    				{
    					"mode": "devdax",
    					"size": 10 * (1<<30),
    					"configlabel": "devdax",
    				},
    				{
    					"mode": "fsdax",
    					"size": 10 * (1<<30),
    					"configlabel": "fsdax",
    				},
    			]	
    		},
            # all the dimms on the disabled socket are treated as block devices (the machine has no other nvmes)
            *[{
         		"PersistentMemoryType": "AppDirectNotInterleaved",
    			"SocketID": "0x0001",
    			"DimmID": dimmid,
    			"namespaces": [
    				{
    					"mode": "fsdax",
    					"size": 100 * (1<<30),
    					"configlabel": "blockdevice", #!!!!!!!!!
    				},
    			]	
            } for dimmid in ["0x1011", "0x1101", "0x1111"]],
    	]
    }, store)

def make_zfs_setup_config(customizations):
    common = {
       "builddir": Path("/home/schwarz/zil-pmem/zil-pmem"),
       "module_args": {
                "zfs": {
                    #"zfs_nocacheflush": "0",
                    #"zfs_zio_taskq_batch_cpu_pct": "1",
                    "zvol_request_sync": "1",
                },
            },
       "pool_properties": {},
       "filesystem_properties": {
           "recordsize": "4k",
           "compression": "off",
       },
       "poolname":"dut",
       "mountpoint": Path("/dut"),
       "vdevs": [
           *[f"nodax:{d}" for d in store.get_all("blockdevice")],
            "log",
       ],
       "create_child_datasets": {
           "dirname_format_str": "ds{}",
           "name_format_str": "ds{}",
           "count": 32,
       },
       "create_child_zvols": {
           "name_format_str": "zv{}",
           "count": 4,
           "size": "4G",
           "volblocksize": 4096,
       },
    }
    return merge_dicts(common, customizations)

def make_fio_config(fio_config_constomizations):
    return merge_dicts({
			#"fio_binary": Path("/usr/local/bin/fio"),
			"fio_binary": Path("/usr/bin/fio"),
            "blocksize": 1<<12, # keep in sync with zfs recordsize prop!
            "size": 200 * (1<<20), # small size so that data fits into the ARC / dbuf caches
            "runtime_seconds": 10,
            "ramp_seconds": 2,
            "fsync_every": 0,
    }, fio_config_constomizations)


def run_fio_series(fio_config, emit_result, numjobs_values=range(1,16), zil_pmem_only_kstats=False, div_size_by_numjobs=False):
    for numjobs in numjobs_values:
        assert "numjobs" not in fio_config
        size = fio_config["size"]
        if div_size_by_numjobs:
            size = int(size/numjobs)
        this_fio_config = merge_dicts(fio_config, { "numjobs": numjobs, "size": size})
        cpu_time_measurement = lib.cpu_stats.CPUTimeMeasurement()

        zil_pmem_only_measurements = {}
        if zil_pmem_only_kstats:
            zil_pmem_only_measurements = {
                "itxg_bypass_stats": lib.zfs_kstats.ZilItxgBypassMeasurement(),
                "zvol_stats": lib.zfs_kstats.ZvolOsLinuxMeasurement(),
                "zil_pmem_stats": lib.zfs_kstats.ZilPmemMeasurement(),
                "zil_pmem_ringbuf_stats": lib.zfs_kstats.ZilPmemRingbufMeasurement(),
            }

        def after_rampup():
            for m in zil_pmem_only_measurements.values():
                m.start()
            cpu_time_measurement.start()

        def after_fio_exit():
            cpu_time_measurement.stop()
            for m in zil_pmem_only_measurements.values():
                m.end()

        fiojson = lib.fio.run(this_fio_config, call_after_rampup=after_rampup, call_after_fio_exit=after_fio_exit)

        result = {
                "fio_config": this_fio_config,
                "fio_jsonplus": fiojson,
                "cpu_time": {
                    "allcpu": cpu_time_measurement.allcpu().to_dict(),
                    "percpu": cpu_time_measurement.percpu().to_dict(orient='records'),
                },
                **{name: m.result().to_dict() for name, m in zil_pmem_only_measurements.items()},
        }
        emit_result(result)


def zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result, run_fio_kwargs={}):
	with lib.zfssetup.setup_openzfs(zfs_setup_config) as _:
            run_fio_series(fio_config, lambda r: emit_result(merge_dicts(r, {"zfs_setup": zfs_setup_config})), **run_fio_kwargs)

def make_fio_zfs_config(fio_customizations):
    return make_fio_config(merge_dicts(
        {
            "target": {
                "type": "fs",
                "filename_format_str": "/dut/ds{}/benchmark", # FIXME no hardcoded paths
                "require_filename_format_str_parent_is_mountpoint": True,
                "prewrite_mode": "delete",
            },
        },
        fio_customizations,
    ))

def zilpmem_zfsconfig(ncommitters=None, zvol_request_sync=None, itxg_bypass=None):
    return make_zfs_setup_config({
        "module_args": {
            "zfs": {
                    "zil_default_kind": "2",
                    "zfs_zil_pmem_prb_ncommitters": ncommitters,
                    "zvol_request_sync": zvol_request_sync,
                    "zfs_zil_itxg_bypass": itxg_bypass,
            }
        },
        "vdevs": [
            "dax:" + store.get_one('fsdax'),
        ],
    })


def zilpmem_benchmark(emit_result):
    zfs_setup_config = zilpmem_zfsconfig(ncommitters="8", zvol_request_sync="0", itxg_bypass="0")
    fio_config = make_fio_zfs_config({"sync": 1})
    zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result)

def itxg_bypass_evaluation(emit_result):
    for (ncommitters, zvol_request_sync, itxg_bypass, fsync_every) in itertools.product([1,2,4,6,7,8,9,10,14,18,24], ["1"], ["1"], [1,8,32]):
        zfs_setup_config = zilpmem_zfsconfig(ncommitters=f"{ncommitters}", zvol_request_sync=zvol_request_sync, itxg_bypass=itxg_bypass)
        fio_config = make_fio_config({
            "target": {
                "type": "blockdev",
                "blockdev_path": Path("/dev/zvol/dut/zv0"), # FIXME no hardcoded paths
            },
            "sync": 0, # we use fsync_every for this
            "fsync_every": fsync_every,
        })
        run_fio_kwargs = {
            "numjobs_values": [1,2,4,6,8,9,10,12,14,16,17,18],
            "zil_pmem_only_kstats": True,
        }
        zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result, run_fio_kwargs=run_fio_kwargs)


def zillwb_configs(fio_customizations):
    zfs_setup_config = make_zfs_setup_config({
        "module_args": {
            "zfs": {
                    "zil_default_kind": "1",
            }
        },
        "vdevs": [
            "nodax:" + store.get_one('fsdax'),
        ],
    })
    fio_config = make_fio_zfs_config(fio_customizations)
    return (zfs_setup_config, fio_config)


def zillwb_benchmark(emit_result):
    zfs_setup_config, fio_config = zillwb_configs({ "sync": 1 })
    zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result)

def sync0_benchmark(emit_result):
    zfs_setup_config, fio_config = zillwb_configs({ "sync": 0 })
    zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result)

def devdax_benchmark(emit_result):
    run_fio_series(make_fio_config({
        "sync": 1,
        "target": {
            "type": "devdax",
            "devdax_path": Path(store.get_one('devdax')),
        }
    }), emit_result)

def fsdax_benchmark(emit_result):
    run_fio_series(make_fio_config({
        "sync": 1,
        "target": {
            "type": "blockdev",
            "blockdev_path": Path(store.get_one('fsdax')),
        }
    }), emit_result)

def ncommitters_scalability(emit_result):
    for ncommitters in [1,2,3,4,5,6,7,8,9,10,11,12,14,16,18]:
        zfs_setup_config = zilpmem_zfsconfig(ncommitters=f"{ncommitters}", zvol_request_sync="0", itxg_bypass="0")
        fio_config = make_fio_zfs_config({"sync": 1, "size": (1<<30)})
        zfs_benchmark_impl(zfs_setup_config, fio_config, emit_result, {"zil_pmem_only_kstats": True, "div_size_by_numjobs": True, "numjobs_values": list(range(1,24))})

def zillwb_latency_analysis(emit_result):

    zfs_setup_config, fio_config_without_numjobs = zillwb_configs({
        "sync": 1,
    })

    with lib.zfssetup.setup_openzfs(zfs_setup_config) as _:

        for numjobs in range(1, 16):
            fio_config = merge_dicts(fio_config_without_numjobs, {"numjobs": numjobs})

            latencyanalysis_config = {
                    "zfs_log_write_kind": "zil-pmem",
            }
            with lib.zfs_write_latencyanalysis.with_bpftrace(latencyanalysis_config) as bpft:
                result_queue = queue.Queue()
                def startstop_measuring():
                    time.sleep(2) # keep in sync with ramp_seconds
                    bpft.start_measuring()
                    time.sleep(8) # keep below runtime_seconds
                    result = bpft.stop_measuring()
                    result_queue.put(result)

                ssmt = threading.Thread(target=startstop_measuring)
                ssmt.start()
                try:
                    lib.fio.run(fio_config)
                except:
                    print("exception running fio")
                    raise
                finally:
                    ssmt.join()
                    result = result_queue.get()
                    assert result != None
                    emit_result(
                        {
                            "zfs_setup": zfs_setup_config,
                            "fio_config": fio_config,
                            "latency_analysis": result,
                        }
                    )

def make_zfs_stacks(identity):
    """all ZFS stacks have recordsize / volblocksize set to 4k"""
    s = lib.storage_stacks
    return [
        s.ZFSLwb(store, identity, "0"),
        s.ZFSLwb(store, identity, "1"),
        *[s.ZFSPmem(store, identity, zvrs, byp, ncom) for zvrs, byp, ncom in itertools.product(["0", "1"], ["0","1"], ["2","3"])],
    ]

def make_blockdev_stacks():
    s = lib.storage_stacks
    return  [
            *make_zfs_stacks("zvol"),
            s.DevPmem(store),
            s.DmWritecache(store), # TODO may want a variant with dm-verity and/or dm-raid as origin?
    ]

def block_device_comparison(emit_result):
    """shoot-out for 4k sync writes for different block device abstractions"""

    stacks = make_blockdev_stacks()

    # run 4k sync write becnhmark on each storage stack
    for sstack_inactive in stacks:
        with sstack_inactive as sstack:
            fio_config = make_fio_config({
                "sync": 1,
                "target": {
                    "type": "blockdev",
                    "blockdev_path": sstack.blockdev_path,
                },
                # TODO: do we want to vary the fio --size parameter?
            })
            def emit_with_stack_info(rd):
                emit_result({
                    "storage_stack": sstack.as_dict(), # includes_id
                    "fio_result": rd,
                })
            run_fio_series(fio_config, emit_with_stack_info)

def fs_comparison(emit_result):
    """shoot-out for various application-level benchmarks for different file system stacks"""


    # setup all fs_stacks
    fs_stacks = []
    linux_filesystems = [lib.storage_stacks.XFS, lib.storage_stacks.Ext4]
    ## linux fs mountpoint
    linux_fs_mountpoint = Path("/dut_linux_fs")
    if linux_fs_mountpoint.exists():
        assert linux_fs_mountpoint.is_dir()
        assert len(list(linux_fs_mountpoint.iterdir())) == 0
    else:
        linux_fs_mountpoint.mkdir()
    ## do it
    for (bdev_stack, linux_fs_cls) in itertools.product(make_blockdev_stacks(), linux_filesystems):
        fs_stacks += [linux_fs_cls(bdev_stack, linux_fs_mountpoint, mount_dax=False)]
        if bdev_stack.is_dax_bdev() and linux_fs_cls.has_o_dax():
            fs_stacks += [linux_fs_cls(bdev_stack, linux_fs_mountpoint, mount_dax=True)]
    ## ZFS is a combined volume manager & file system => include it in the benchmark in its fs role
    fs_stacks.extend(make_zfs_stacks(identity="zfs"))

    # setup all benchmarks
    variable_values = list(range(1,17))
    benchmarks = [
        lib.benchmarks.Dummy(), # verify that all the storage stack impls work

        # Emulates I/O activity of a simple mail server that stores each e-mail in a separate file (/var/mail/ server). The workload consists of a multi-threaded set of create-append-sync, read-append-sync, read and delete operations in a single directory. 16 threads are used by default. The workload generated is somewhat similar to Postmark but multi-threaded. 
        # https://github.com/filebench/filebench/wiki/Predefined-personalities
        #
        # metadata intensive according to Son et al., “High-Performance Transaction Processing in Journaling File Systems.”
        lib.benchmarks.Filebench(identity="filebench-varmail", workload="varmail", vars={"nthreads": variable_values}),

#        # oltp workload
#        #A database emulator. This workload performs file system operations using Oracle 9i I/O model. It tests the performance of small random reads and writes, and is sensitive to the latency of moderate size (128k+) synchronous writes to the log file. By default launches 200 reader processes, 10 processes for asynchronous writing, and a log writer. The emulation includes the use of ISM shared memory as per Oracle, Sybase, etc. which is critical to I/O efficiency.
#        # https://github.com/filebench/filebench/wiki/Predefined-personalities
        lib.benchmarks.Filebench(identity="filebench-oltp", workload="oltp", vars={"ndbwriters": variable_values}),

        # rocksdb db_bench-inspired benchmark for sqlite, with WAL enabled, doesn't have parallelism params
        lib.benchmarks.SqliteBench(num=10_000_000),

        # rocksdb db_bench fillsync benchmark (mostly a stress-test for the rocksdb WAL)
        lib.benchmarks.RocksdbBench(num=400_000, nthreads=variable_values),

        # redis-benchmark 'SET' test, redis confired appendonly yes, appendfsync always (="every redis operation is durable")
        lib.benchmarks.RedisSetBench(nthreads_nclients=variable_values),

        # sysbench oltp_insert workload against MariaDB 10.5.9 docker container
        # (net=host => localhost TCP+IP)
        # (/var/lib/mysql on pmem partition (both innodb tables and WAL on same partition)
        lib.benchmarks.MariaDbSysbenchOltpInsert(nthreads=variable_values),

        # fio benchmark
        lib.benchmarks.Fio4kSyncRandFsWrite(numjobs_values=variable_values),
    ]

    # for each fs stack: set it up, then run each experiment on it
    # (setting up storage stacks takes some time => re-use stack for all experiments)
    for fs_stack in fs_stacks:
        with fs_stack as fs_stack:
            for bench_i, bench in enumerate(benchmarks):
                # create a separate directory within the mountpoint so that benchmarks don't interfere
                bench_dir = fs_stack.fsstack_mountpoint / f"{bench_i}"
                assert not bench_dir.exists()
                bench_dir.mkdir()

                def emit_with_storage_stack(rd):
                    emit_result({
                        "storage_stack": fs_stack.as_dict(), # includes id
                        "result": rd, # includes bench id
                    })
                bench.run(bench_dir, emit_with_storage_stack)

################# MAIN SCRIPT ##################

system_setup__i30pc62()

print(store.to_dict())

resultdir = Path("./results")
result_storage = ResultStorage(resultdir)

def save_json_result(subject, prefix, result_dict):
    result_dict = merge_dicts(result_dict, {"test_subject": subject})
    result_storage.save_json_result(prefix, result_dict)

## perf benchmarks
#zilpmem_benchmark(lambda rd: save_json_result("zil-pmem", "perf", rd))
#zillwb_benchmark(lambda rd: save_json_result("zil-lwb", "perf", rd))
#sync0_benchmark(lambda rd: save_json_result("async", "perf", rd))
#devdax_benchmark(lambda rd: save_json_result("devdax", "perf", rd))
#fsdax_benchmark(lambda rd: save_json_result("fsdax", "perf", rd))
#
## latency breakdown
#zillwb_latency_analysis(lambda rd: save_json_result(None, "latency_analysis", rd))
#
## itxg bypass
#itxg_bypass_evaluation(lambda rd: result_storage.save_json_result("itxg_bypass_v6", rd))
#
## block-device performance
#block_device_comparison(lambda rd: result_storage.save_json_result("blockdev_comparison_v1", rd))
#
ncommitters_scalability(lambda rd: result_storage.save_json_result("ncommmitters_scalability_i60pc62__v2", rd))

## file-system application benchmarks
#fs_comparison(lambda rd: result_storage.save_json_result("fs_comparison_v1", rd))

#def zil_lwb_setup():
#    return make_zfs_setup_config({
#            "module_args": {
#                "zfs": {
#                        "zil_default_kind": "1",
#                        "zvol_request_sync": "0",
#                        "zfs_zil_itxg_bypass": "0",
#                }
#            },
#            "vdevs": [
#                "nodax:" + store.get_one('fsdax'), #!
#            ],
#        })
#
#setup = {
#        "zil-pmem": merge_dicts(zilpmem_zfsconfig(zvol_request_sync=sys.argv[2], itxg_bypass=sys.argv[3]), {"module_args": {"zfs": {"zfs_zil_pmem_prb_ncommitters": sys.argv[4]}}}),
#        "zil-lwb": zil_lwb_setup(),
#        }[sys.argv[1]]
#
#def run_with_setup(setup):
#    with contextlib.ExitStack() as stack:
#        #_ = stack.enter_context(lib.zfssetup.setup_openzfs(setup))
#        #mountpoint = Path("/dut/ds0")
#        mountpoint = stack.enter_context(setup())
#        assert mountpoint.is_mount()
#
#        # varmail, variable is 'nthreads'
#        # metadata intensive according to Son et al., “High-Performance Transaction Processing in Journaling File Systems.”
#        # Emulates I/O activity of a simple mail server that stores each e-mail in a separate file (/var/mail/ server). The workload consists of a multi-threaded set of create-append-sync, read-append-sync, read and delete operations in a single directory. 16 threads are used by default. The workload generated is somewhat similar to Postmark but multi-threaded. 
#        # https://github.com/filebench/filebench/wiki/Predefined-personalities
#        # TODO probably need longer runtime!
#        #res = lib.filebench.run({
#        #    "filebench_binary": Path("/usr/local/bin/filebench"),
#        #    "name": "varmail",
#        #    "dir": mountpoint,
#        #    "runtime_secs": 10,
#        #    "vars": {
#        #        "nthreads": 7,
#        #    },
#        #})
#
#        # fileserver workload
#        # 'data intensive' according to Son et al., “High-Performance Transaction Processing in Journaling File Systems.”
#        # but it doesn't use sync I/O
#        #res = lib.filebench.run({
#        #    "filebench_binary": Path("/usr/local/bin/filebench"),
#        #    "name": "fileserver",
#        #    "dir": mountpoint,
#        #    "runtime_secs": 10,
#        #    "vars": {
#        #        "nthreads": 1,
#        #    },
#        #})
#
#        # oltp workload
#        #A database emulator. This workload performs file system operations using Oracle 9i I/O model. It tests the performance of small random reads and writes, and is sensitive to the latency of moderate size (128k+) synchronous writes to the log file. By default launches 200 reader processes, 10 processes for asynchronous writing, and a log writer. The emulation includes the use of ISM shared memory as per Oracle, Sybase, etc. which is critical to I/O efficiency.
#        # https://github.com/filebench/filebench/wiki/Predefined-personalities
#        #res = lib.filebench.run({
#        #    "filebench_binary": Path("/usr/local/bin/filebench"),
#        #    "name": "oltp",
#        #    "dir": mountpoint,
#        #    "runtime_secs": 10,
#        #    "vars": {
#        #        "ndbwriters": 32,
#        #    },
#        #})
#
#        # sqlite fillrandsync benchmark
#        # can't vary number of threads :/
#        #res = lib.sqlite_bench.run({
#        #    "sqlitebench_checkout": Path("/root/src/sqlite-bench"),
#        #    "db_dir": mountpoint,
#        #    "benchmark": "fillrandsync",
#        #    "num": 10_000_000,
#        #})
#
#        # rocksdb db_bench fillsync
#        # (this is essentially a micro-benchmark of rocksdb WAL perf)
#        #res = lib.rocksdb_bench.run({
#        #    "rocksdb_dir": Path("/root/src/rocksdb"),
#        #    "db_dir": mountpoint,
#        #    "wal_dir": mountpoint,
#        #    "num": 400_000,
#        #    "threads": 8,
#        #    "benchmark": "fillsync",
#        #    "sync": True,
#        #})
#
#        #config = {
#        #    "redis6_checkout": Path("/root/src/redis"),
#        #    "dir": mountpoint,
#        #    #"nrequests": {
#        #    #    "type": "estimate",
#        #    #    "target_runtime_secs": 20,
#        #    #    "estimate_nrequests": 100_000,
#        #    #    "actual_runtime_tolerance_bounds": (15, 40),
#        #    #},
#        #    "nrequests": {
#        #        "type": "fixed",
#        #        "nrequests": 1_000_000,
#        #    },
#        #    "keyspacelen": 'nrequests',
#        #    "pipeline_numreq": 1,
#        #    "datasize": 3,
#        #    "clients": 1,
#        #    "threads": 1,
#        #}
#        #res = lib.redis_benchmark.run(config)
#
#        print(mountpoint)
#
#        #res = lib.sysbench_mariadb.run({
#        #    "dir": mountpoint,
#        #    "mariadb": {
#        #        "docker_image": {
#        #            "repository": "mariadb",
#        #            "tag": "10.5.9",
#        #        },
#        #    },
#        #    "sysbench_checkout": Path("/root/src/sysbench"),
#        #    "threads": 1,
#        #    "runtime_secs": 10,
#        #})
#
#        import pdb
#        pdb.set_trace()
#
#        print(res)
#
#        return {} 
#
#def make_openzfs_ctxmanager(config):
#    @contextlib.contextmanager
#    def f():
#        with lib.zfssetup.setup_openzfs(config):
#            yield Path("/dut/ds1")
#    return f
#
#def make_linux_filesystem_ctxmanager(Cl, config):
#    @contextlib.contextmanager
#    def f():
#        with Cl(**config):
#            yield config['mountpoint']
#    return f
#
#linux_filesystem_mountpoint = Path("/mnt")
#setups = [
#    ("zil-lwb", make_openzfs_ctxmanager(zil_lwb_setup())),
#    ("zil-pmem", make_openzfs_ctxmanager(setup)),
#    ("xfs-dax", make_linux_filesystem_ctxmanager(lib.linux_filesystem.XFS, {"blockdev": Path(store.get_one('fsdax')), "mountpoint":linux_filesystem_mountpoint, "mount_args":["-o", "dax"]})),
#    ("ext4-dax", make_linux_filesystem_ctxmanager(lib.linux_filesystem.Ext4, {"blockdev": Path(store.get_one('fsdax')), "mountpoint":linux_filesystem_mountpoint, "mount_args":["-o", "dax"]})),
#]
#
#results = []
#for setup in setups[3:]:
#    res = run_with_setup(setup[1])
#    results += [(setup[0], res)]
#
#import pdb
#pdb.set_trace()
#
#print(results)
#print("json")
#print("")
#print(json.dumps(results, default=json_dump_default_to_str))
#print("")
#print(json.dumps([(e, r['metrics']) for e, r in results]))

#print({
#    "baseline": baseline['metrics'],
#    "bench": bench['metrics']
#})


#with contextlib.ExitStack() as stack:
#    dm_pmem = stack.enter_context(lib.devicemapper.Target(name="pmem", table=lib.devicemapper.simple_linear_table({
#        "size": 10*(1<<30),
#        "device": Path("/dev/pmem0.2")})))
#    dm_wc = stack.enter_context(lib.devicemapper.WritecachePmem({
#            "name": "wc",
#            "size": 40*(1<<30),
#            "blocksize": 4096,
#            "origin_device": Path("/dev/nvme1n1p1"),
#            "cache_device": dm_pmem.path,
#            "options": {
#                "low_watermark": 0,
#                "high_watermark": 0,
#            }
#        }))
#    fs = stack.enter_context(lib.linux_filesystem.XFS(**{"blockdev": dm_wc.path, "mountpoint":Path("/mnt")}))
#    import pdb
#    pdb.set_trace()

#class Zvol:
#    def __init__(self):
#        self.open_setup = None
#
#    @property
#    def blockdev_path(self):
#        if not self.open_setup:
#            return None
#        return Path("/dev/zvol/dut/zv0") # FIXME
#
#    def __enter__(self):
#        config = self._make_config()
#        assert self.blockdev_path is None
#        assert self.open_setup is None
#        self.open_setup = lib.zfssetup.setup_openzfs(config)
#        self.open_setup.__enter__()
#        return self
#
#    def __exit__(self, type, val, bt):
#        self.open_setup.__exit__(type, val, bt)
#        self.open_setup = None
#
#class ZvolLwb(Zvol):
#
#    def __init__(self, zvol_request_sync):
#        super().__init__()
#        self.zvol_request_sync = zvol_request_sync
#
#    def cli_name(self):
#        return f"zvol-lwb-rs_{self.zvol_request_sync}"
#
#    def _make_config(self):
#        return make_zfs_setup_config({
#            "module_args": {
#                "zfs": {
#                        "zil_default_kind": "1",
#                        "zvol_request_sync": self.zvol_request_sync,
#                }
#            },
#            "vdevs": [
#                # "nodax:" prefix needs to go if we switch this to mainline openzfs
#                "nodax:" + store.get_one('fsdax'),
#            ],
#        })
#
#class ZvolPmem(Zvol):
#
#    def __init__(self, zvol_request_sync, itxg_bypass, ncommitters):
#        super().__init__()
#        self.zvol_request_sync = zvol_request_sync
#        self.ncommitters = ncommitters
#        self.zfs_zil_itxg_bypass = itxg_bypass
#
#    def cli_name(self):
#        return f"zvol-pmem-rs_{self.zvol_request_sync}-byp_{self.zfs_zil_itxg_bypass}-nc_{self.ncommitters}"
#
#    def _make_config(self):
#        return make_zfs_setup_config({
#            "module_args": {
#                "zfs": {
#                        "zil_default_kind": "2",
#                        "zfs_zil_pmem_prb_ncommitters": self.ncommitters,
#                        "zvol_request_sync": self.zvol_request_sync,
#                        "zfs_zil_itxg_bypass": self.itxg_bypass,
#                }
#            },
#            "vdevs": [
#                "dax:" + store.get_one('fsdax'),
#            ],
#        })
#
#
#class DevPmem:
#
#    def __init__(self):
#        self.dev = None
#
#    def cli_name(self):
#        return f"devpmem"
#
#    @property
#    def blockdev_path(self):
#        return self.dev
#
#    def __enter__(self):
#        assert self.dev is None
#        self.dev = Path(store.get_first("fsdax"))
#        assert self.dev.is_block_device()
#
#    def __exit__(self):
#        assert self.dev is not None
#        self.dev = None
#
#class DmWritecache:
#
#    def __init__(self):
#        self.stack = None
#        self.dm_wc = None
#
#    def cli_name(self):
#        return f"dm-writecache"
#
#    @property
#    def blockdev_path(self):
#        if not self.dm_wc:
#            return None
#        return self.dm_wc.path
#
#    def __enter__(self):
#        assert self.stack is None
#        assert self.dm_wc is None
#        self.stack = contextlib.ExitStack()
#        dm_pmem = self.stack.enter_context(lib.devicemapper.Target(name="pmem", table=lib.devicemapper.simple_linear_table({
#            "size": 10*(1<<30),
#            "device": Path(store.get_one("fsdax"))})))
#        self.dm_wc = self.stack.enter_context(lib.devicemapper.WritecachePmem({
#                "name": "wc",
#                "size": 40*(1<<30),
#                "blocksize": 4096,
#                "origin_device": Path(store.get_first("blockdevice")),
#                "cache_device": dm_pmem.path,
#                # always do writeback so that it approximates the zvol
#                "options": {
#                    "low_watermark": 0,
#                    "high_watermark": 0,
#                }
#            }))
#
#        return self
#
#    def __exit__(self, type, val, bt):
#        self.stack.__exit__(type, val, bt)
#        self.stack = None
#        self.dm_wc = None
#
#zvol_pmem_ncommitters = 3
#blockdev_stacks = [
#   [ZvolLwb, 0],
#   [ZvolLwb, 1],
#
#   [ZvolPmem, 0, 0, 3],
#   [ZvolPmem, 0, 1, 3],
#   [ZvolPmem, 0, 2, 3],
#
#   [ZvolPmem, 1, 0, 3],
#   [ZvolPmem, 1, 1, 3],
#   [ZvolPmem, 1, 2, 3],
#
#   [DevPmem],
#   [DmWritecache],
#]
#import collections
#
#blockdev_stacks = [collections.deque(t) for t in blockdev_stacks]
#
#blockdev_stacks = [(t.popleft())(*t) for t in blockdev_stacks]
#
#blockdev_stacks_by_name = { s.cli_name(): s for s in blockdev_stacks}
#
#import click
#
#@click.group()
#def app():
#    pass
#
#@app.command()
#@click.argument("stackname", type=click.Choice(sorted(list(blockdev_stacks_by_name.keys()))))
#def blockdev_stack(stackname):
#    try:
#        s = blockdev_stacks_by_name[stackname]
#    except KeyError as e:
#        print(f"invalid stack name {stackname!r}. available: {sorted(list(blockdev_stacks_by_name.keys()))}")
#
#    print(f"setting up stack {s.cli_name()!r}")
#
#    with s as s:
#        print(f"blockdev is available at {s.blockdev_path}")
#        import pdb
#        pdb.set_trace()
#
#class ZfsStack:
#    pass
#
#class LinuxFilesystemStack:
#    def __init__(self, blockdev_stack):
#        self.blockdev_stack = blockdev_stack
#
#    def __enter__(self):
#
#
#@app.command()
#def fs_stack(stackname):
#
#app()

#with contextlib.ExitStack() as stack:
#    dm_pmem = stack.enter_context(lib.devicemapper.Target(name="pmem", table=lib.devicemapper.simple_linear_table({
#        "size": 10*(1<<30),
#        "device": Path("/dev/pmem0.2")})))
#    dm_wc = stack.enter_context(lib.devicemapper.WritecachePmem({
#            "name": "wc",
#            "size": 40*(1<<30),
#            "blocksize": 4096,
#            "origin_device": Path("/dev/nvme1n1p1"),
#            "cache_device": dm_pmem.path,
#            "options": {
#                "low_watermark": 0,
#                "high_watermark": 0,
#            }
#        }))
#
#    mountpoint = Path("/mnt")
#    fs = stack.enter_context(lib.linux_filesystem.XFS(**{"blockdev": dm_wc.path, "mountpoint":mountpoint}))
#
#    res = lib.rocksdb_bench.run({
#        "rocksdb_dir": Path("/root/src/rocksdb"),
#        "db_dir": mountpoint,
#        "wal_dir": mountpoint,
#        "num": 100000,
#        "threads": 3,
#        "benchmark": "fillsync",
#        "sync": True,
#    })
#
#    import pdb
#    pdb.set_trace()


